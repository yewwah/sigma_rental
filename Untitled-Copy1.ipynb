{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sb \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [300]\n",
    "learning = [0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    if type(x) == list:\n",
    "        return ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "#df = df[:20]\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, max_features):\n",
    "        self._vectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._column = column\n",
    "        \n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        if self._column == 'features':\n",
    "            df[self._column] = df[self._column].apply(check)\n",
    "            #f[self._column]\n",
    "        self._vectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        if self._column == 'features':\n",
    "            df[self._column] = df[self._column].apply(check)\n",
    "        return self._vectorizer.transform(df[self._column].apply(self._clean_html_tags)).todense()\n",
    "#        return self._vectorizer.transform(df[self._column])\n",
    "\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        print 'col'\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return df[self.cols].values\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class DateExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, day = False, month= False, year = False):\n",
    "        self.cols = cols\n",
    "        self.day = day\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "        self.index = None\n",
    "        \n",
    "    def transform(self, df, y = None):\n",
    "        frame = self._fit_date(df[self.cols])\n",
    "        cols_names = frame.columns.values\n",
    "        \n",
    "        #get the columns that interesect to account for unseen labels\n",
    "        intersect = set.intersection(set(cols_names), set(self.index))\n",
    "        \n",
    "        #get the differences to account for unseen labels\n",
    "        diff = set.difference(set(self.index), set(cols_names))\n",
    "        frame = frame[list(intersect)]\n",
    "        frame = pd.concat((frame, pd.DataFrame(columns = list(diff))))\n",
    "        frame.fillna(0, inplace=True)\n",
    "        return frame\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = self._fit_date(X[self.cols])\n",
    "        self.index = df.columns.values\n",
    "        return self\n",
    "    \n",
    "    def _fit_date(self, X):\n",
    "        if self.day:\n",
    "            prefix = 'day'\n",
    "            when = pd.DatetimeIndex(X).day\n",
    "        elif self.month:\n",
    "            prefix = 'month'\n",
    "            when = pd.DatetimeIndex(X).month\n",
    "        else:\n",
    "            prefix = 'year'\n",
    "            when = pd.DatetimeIndex(X).year\n",
    "        \n",
    "        frame = pd.get_dummies(when, prefix = prefix)\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col\n",
      "starting code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yewwah-ubuntu/.local/lib/python2.7/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/yewwah-ubuntu/.local/lib/python2.7/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n",
      "Log loss from sklearn 0.599790307962\n"
     ]
    }
   ],
   "source": [
    "#pg = dict(clf__verbose = ['True'])\n",
    "pg = {'clf__learning_rate' : learning, 'clf__n_estimators' : estimators}\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "#from Transformers import TextTransformer, ColumnExtractor\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in skf.split(df, df['interest_level']):\n",
    "    a = TextTransformer('description', max_features=3000)\n",
    "    b = TextTransformer('features', max_features=3000)\n",
    "    c = TextTransformer('street_address', max_features = 3000)\n",
    "    d = TextTransformer('display_address', max_features = 3000)\n",
    "    pipeline = Pipeline([\n",
    "        ('test', FeatureUnion\n",
    "         ([\n",
    "            ('description', a ), # can pass in either a pipeline\n",
    "            ('features', b),\n",
    "            ('street', c),\n",
    "            ('display', d),\n",
    "            ('lat_long', ColumnExtractor(['latitude', 'longitude', 'price', 'bedrooms', 'bathrooms'])),\n",
    "            ('year', DateExtractor('created', year = True)),\n",
    "            ('month', DateExtractor('created', month = True)),\n",
    "            ('day', DateExtractor('created', day = True))\n",
    "                    \n",
    "        ])),\n",
    "    ('clf',xgboost.XGBClassifier(silent = False, n_estimators = 300, learning_rate = 0.2, max_depth = 5))\n",
    "    #('clf',SVC(probability = True))\n",
    "    ])\n",
    "    print 'starting code'\n",
    "    train, train_labels = df.iloc[train_index], df['interest_level'].iloc[train_index]\n",
    "    test, test_labels = df.iloc[test_index], df['interest_level'].iloc[test_index]\n",
    "    model = pipeline.fit(train,train_labels)\n",
    "    print 'finished training'\n",
    "    pred = (model.predict_proba(test))\n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    break\n",
    "#scores = cross_val_score(pipeline, df, df['interest_level'], cv=2)\n",
    "#regr = \n",
    "#search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "#pg = {'clf__C': [0.1,0.2]}\n",
    "\n",
    "#print pipeline.get_params().keys()\n",
    "#grid = GridSearchCV(pipeline, param_grid = pg, cv = 2, verbose = 10)\n",
    "#grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://sealagreements.com/leasing/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03587535,  0.55462253,  0.40950212],\n",
       "       [ 0.15603963,  0.6288799 ,  0.2150805 ],\n",
       "       [ 0.04141739,  0.67313004,  0.28545257],\n",
       "       ..., \n",
       "       [ 0.02233118,  0.84804994,  0.12961885],\n",
       "       [ 0.40041694,  0.16948836,  0.43009472],\n",
       "       [ 0.03334413,  0.85002995,  0.11662594]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json('data/test.json')\n",
    "pred = (model.predict_proba(test_df))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(pred, columns = ['low', 'medium', 'high'])\n",
    "output['listing_id'] = test_df['listing_id'].values\n",
    "output = output[['listing_id', 'low', 'medium','high']]\n",
    "output.to_csv('predictions.csv', header = ['listing_id', 'low', 'medium','high'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_transform(content_lst):\n",
    "    return ' '.join(content_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "param_grid = dict(learning_rate = learning, n_estimators = estimators)\n",
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(silent = False)\n",
    "    search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "    res = search.fit(train, train_labels)\n",
    "    results.append(res)\n",
    "\n",
    "    #regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (res.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(n_estimators  = 300, silent = False)\n",
    "    regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (regr.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred['listing_id'] = listing_ids.values\n",
    "pred = pred[['listing_id', 'high', 'medium', 'low']]\n",
    "pred.to_csv('test_raw_xgboost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10           Metropolitan \n",
       "10000            Columbus \n",
       "100004               W 13 \n",
       "100007          East 49th \n",
       "100013         West 143rd \n",
       "100014          West 18th \n",
       "100016         West 107th \n",
       "100020          West 21st \n",
       "100026    Hamilton Terrace\n",
       "100027          522 E 11th\n",
       "100030               York \n",
       "10004            W. 173rd \n",
       "100044           E 38th St\n",
       "100048          West 63rd \n",
       "10005       East 56th St..\n",
       "100051          East 34th \n",
       "100052            1st Ave.\n",
       "100053           Thayer St\n",
       "100055         West 106th \n",
       "100058                1st \n",
       "Name: new, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new'] = df['display_address'].apply(lambda x: x.replace('Street', '').replace('Avenue', ''))\n",
    "df['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = regr.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epochs')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================================\n",
    "Concatenating multiple feature extraction methods\n",
    "=================================================\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a\n",
    "dataset. Often it is beneficial to combine several methods to obtain good\n",
    "performance. This example shows how to use ``FeatureUnion`` to combine\n",
    "features obtained by PCA and univariate selection.\n",
    "\n",
    "Combining features using this transformer has the benefit that it allows\n",
    "cross validation and grid searches over the whole process.\n",
    "\n",
    "The combination used in this example is not particularly helpful on this\n",
    "dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print iris.data\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "#param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  #features__univ_select__k=[1, 2],\n",
    "                  #svm__C=[0.1, 1, 10])\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,col, max_features = 10):\n",
    "        self.tfidfVectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._vectorizer = None\n",
    "        self._column = 'description'\n",
    "\n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        self._vectorizer = self.tfidfVectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return self._vectorizer.transform(df[self._column]).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "len(df)\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "a = TextTransformer('testing')\n",
    "b = TextTransformer('features', max_features=10)\n",
    "a = FeatureUnion([(\"pca\", a), ('test', b)])\n",
    "pipe = Pipeline([('description', a)])\n",
    "X = df\n",
    "y = df['interest_level'].values\n",
    "pipe.fit(X,y)\n",
    "# pg = {'clf__C': [0.1,1]}\n",
    "# grid = GridSearchCV(pipeline, param_grid= pg ,cv = 2)\n",
    "# grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df[['latitude', 'longitude', 'interest_level']]\n",
    "a['interest_level'] = a['interest_level'].apply(repl)\n",
    "#a = a.pivot_table('latitude', 'longitude', 'interest_level', aggfunc='sum')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_lat = np.max(df['latitude'])\n",
    "min_lat = np.min(df['latitude'])\n",
    "mean = np.mean(df['latitude'])\n",
    "std =  np.std(df['latitude'])\n",
    "width = 3 * std\n",
    "(n, bin, patch) = plt.hist(df['latitude'], bins = [min_lat, mean - width, mean , mean + width, max_lat])\n",
    "print n \n",
    "print bin\n",
    "print patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df[['latitude', 'longitude', 'interest_level']]\n",
    "df1.head\n",
    "df2 = df1[df1['latitude'] >41]\n",
    "sns.boxplot(y=df2['latitude'], data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('latitude', 'longitude', data = df1, hue = 'interest_level', fit_reg = False, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1[ (df1['longitude'] > -74.1) & (df1['longitude'] <-73.8)]\n",
    "df1 = df1[(df1['latitude'] > 40.5) & (df1['latitude'] <40.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repl(label):\n",
    "    if label == 'low':\n",
    "        return 1\n",
    "    elif label == 'medium':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['latitude'] = pd.cut(df['latitude'], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['longitude'] = pd.cut(df['longitude'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
