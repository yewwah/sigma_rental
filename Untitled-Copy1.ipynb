{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sb \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [300]\n",
    "learning = [0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "df = df[:20]\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, max_features):\n",
    "        self.tfidfVectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._vectorizer = None\n",
    "        self._column = column\n",
    "\n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        if self._column == 'features':\n",
    "            df[self._column] = df[self._column].apply(lambda x : ' '.join(x))\n",
    "        self._vectorizer = self.tfidfVectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return self._vectorizer.transform(df[self._column])\n",
    "\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return df[self.cols].values\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class DateExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, day = False, month= False, year = False):\n",
    "        self.cols = cols\n",
    "        self.day = day\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "        self.index = None\n",
    "        \n",
    "    def transform(self, df, y = None):\n",
    "        frame = self._fit_date(df[self.cols])\n",
    "        cols_names = frame.columns.values\n",
    "        \n",
    "        #get the columns that interesect to account for unseen labels\n",
    "        interesect = set.intersection(set(cols_names), set(self.index))\n",
    "        \n",
    "        #get the differences to account for unseen labels\n",
    "        diff = set.difference(set(self.index), set(cols_names))\n",
    "        frame = frame[list(interesect)]\n",
    "        frame = pd.concat((frame, pd.DataFrame(columns = list(diff))))\n",
    "        frame.fillna(0, inplace=True)\n",
    "        return frame\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = self._fit_date(X[self.cols])\n",
    "        self.index = df.columns.values\n",
    "        return self\n",
    "    \n",
    "    def _fit_date(self, X):\n",
    "        if self.day:\n",
    "            prefix = 'day'\n",
    "            when = pd.DatetimeIndex(X).day\n",
    "        elif self.month:\n",
    "            prefix = 'month'\n",
    "            when = pd.DatetimeIndex(X).month\n",
    "        else:\n",
    "            prefix = 'year'\n",
    "            when = pd.DatetimeIndex(X).year\n",
    "        \n",
    "        frame = pd.get_dummies(when, prefix = prefix)\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>building_id</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>display_address</th>\n",
       "      <th>features</th>\n",
       "      <th>interest_level</th>\n",
       "      <th>latitude</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>manager_id</th>\n",
       "      <th>photos</th>\n",
       "      <th>price</th>\n",
       "      <th>street_address</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
       "      <td>2016-06-24 07:54:24</td>\n",
       "      <td>A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...</td>\n",
       "      <td>Metropolitan Avenue</td>\n",
       "      <td>[]</td>\n",
       "      <td>medium</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>7211212</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>5ba989232d0489da1b5f2c45f6688adc</td>\n",
       "      <td>[https://photos.renthop.com/2/7211212_1ed4542e...</td>\n",
       "      <td>3000</td>\n",
       "      <td>792 Metropolitan Avenue</td>\n",
       "      <td>Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>c5c8a357cba207596b04d1afd1e4f130</td>\n",
       "      <td>2016-06-12 12:19:27</td>\n",
       "      <td></td>\n",
       "      <td>Columbus Avenue</td>\n",
       "      <td>[Doorman, Elevator, Fitness Center, Cats Allow...</td>\n",
       "      <td>low</td>\n",
       "      <td>40.7947</td>\n",
       "      <td>7150865</td>\n",
       "      <td>-73.9667</td>\n",
       "      <td>7533621a882f71e25173b27e3139d83d</td>\n",
       "      <td>[https://photos.renthop.com/2/7150865_be3306c5...</td>\n",
       "      <td>5465</td>\n",
       "      <td>808 Columbus Avenue</td>\n",
       "      <td>Columbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>c3ba40552e2120b0acfc3cb5730bb2aa</td>\n",
       "      <td>2016-04-17 03:26:41</td>\n",
       "      <td>Top Top West Village location, beautiful Pre-w...</td>\n",
       "      <td>W 13 Street</td>\n",
       "      <td>[Laundry In Building, Dishwasher, Hardwood Flo...</td>\n",
       "      <td>high</td>\n",
       "      <td>40.7388</td>\n",
       "      <td>6887163</td>\n",
       "      <td>-74.0018</td>\n",
       "      <td>d9039c43983f6e564b1482b273bd7b01</td>\n",
       "      <td>[https://photos.renthop.com/2/6887163_de85c427...</td>\n",
       "      <td>2850</td>\n",
       "      <td>241 W 13 Street</td>\n",
       "      <td>W 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28d9ad350afeaab8027513a3e52ac8d5</td>\n",
       "      <td>2016-04-18 02:22:02</td>\n",
       "      <td>Building Amenities - Garage - Garden - fitness...</td>\n",
       "      <td>East 49th Street</td>\n",
       "      <td>[Hardwood Floors, No Fee]</td>\n",
       "      <td>low</td>\n",
       "      <td>40.7539</td>\n",
       "      <td>6888711</td>\n",
       "      <td>-73.9677</td>\n",
       "      <td>1067e078446a7897d2da493d2f741316</td>\n",
       "      <td>[https://photos.renthop.com/2/6888711_6e660cee...</td>\n",
       "      <td>3275</td>\n",
       "      <td>333 East 49th Street</td>\n",
       "      <td>East 49th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-28 01:32:41</td>\n",
       "      <td>Beautifully renovated 3 bedroom flex 4 bedroom...</td>\n",
       "      <td>West 143rd Street</td>\n",
       "      <td>[Pre-War]</td>\n",
       "      <td>low</td>\n",
       "      <td>40.8241</td>\n",
       "      <td>6934781</td>\n",
       "      <td>-73.9493</td>\n",
       "      <td>98e13ad4b495b9613cef886d79a6291f</td>\n",
       "      <td>[https://photos.renthop.com/2/6934781_1fa4b41a...</td>\n",
       "      <td>3350</td>\n",
       "      <td>500 West 143rd Street</td>\n",
       "      <td>West 143rd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bathrooms  bedrooms                       building_id  \\\n",
       "10            1.5         3  53a5b119ba8f7b61d4e010512e0dfc85   \n",
       "10000         1.0         2  c5c8a357cba207596b04d1afd1e4f130   \n",
       "100004        1.0         1  c3ba40552e2120b0acfc3cb5730bb2aa   \n",
       "100007        1.0         1  28d9ad350afeaab8027513a3e52ac8d5   \n",
       "100013        1.0         4                                 0   \n",
       "\n",
       "                    created  \\\n",
       "10      2016-06-24 07:54:24   \n",
       "10000   2016-06-12 12:19:27   \n",
       "100004  2016-04-17 03:26:41   \n",
       "100007  2016-04-18 02:22:02   \n",
       "100013  2016-04-28 01:32:41   \n",
       "\n",
       "                                              description  \\\n",
       "10      A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...   \n",
       "10000                                                       \n",
       "100004  Top Top West Village location, beautiful Pre-w...   \n",
       "100007  Building Amenities - Garage - Garden - fitness...   \n",
       "100013  Beautifully renovated 3 bedroom flex 4 bedroom...   \n",
       "\n",
       "            display_address  \\\n",
       "10      Metropolitan Avenue   \n",
       "10000       Columbus Avenue   \n",
       "100004          W 13 Street   \n",
       "100007     East 49th Street   \n",
       "100013    West 143rd Street   \n",
       "\n",
       "                                                 features interest_level  \\\n",
       "10                                                     []         medium   \n",
       "10000   [Doorman, Elevator, Fitness Center, Cats Allow...            low   \n",
       "100004  [Laundry In Building, Dishwasher, Hardwood Flo...           high   \n",
       "100007                          [Hardwood Floors, No Fee]            low   \n",
       "100013                                          [Pre-War]            low   \n",
       "\n",
       "        latitude  listing_id  longitude                        manager_id  \\\n",
       "10       40.7145     7211212   -73.9425  5ba989232d0489da1b5f2c45f6688adc   \n",
       "10000    40.7947     7150865   -73.9667  7533621a882f71e25173b27e3139d83d   \n",
       "100004   40.7388     6887163   -74.0018  d9039c43983f6e564b1482b273bd7b01   \n",
       "100007   40.7539     6888711   -73.9677  1067e078446a7897d2da493d2f741316   \n",
       "100013   40.8241     6934781   -73.9493  98e13ad4b495b9613cef886d79a6291f   \n",
       "\n",
       "                                                   photos  price  \\\n",
       "10      [https://photos.renthop.com/2/7211212_1ed4542e...   3000   \n",
       "10000   [https://photos.renthop.com/2/7150865_be3306c5...   5465   \n",
       "100004  [https://photos.renthop.com/2/6887163_de85c427...   2850   \n",
       "100007  [https://photos.renthop.com/2/6888711_6e660cee...   3275   \n",
       "100013  [https://photos.renthop.com/2/6934781_1fa4b41a...   3350   \n",
       "\n",
       "                 street_address            new  \n",
       "10      792 Metropolitan Avenue  Metropolitan   \n",
       "10000       808 Columbus Avenue      Columbus   \n",
       "100004          241 W 13 Street          W 13   \n",
       "100007     333 East 49th Street     East 49th   \n",
       "100013    500 West 143rd Street    West 143rd   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10           Metropolitan \n",
       "10000            Columbus \n",
       "100004               W 13 \n",
       "100007          East 49th \n",
       "100013         West 143rd \n",
       "100014          West 18th \n",
       "100016         West 107th \n",
       "100020          West 21st \n",
       "100026    Hamilton Terrace\n",
       "100027          522 E 11th\n",
       "100030               York \n",
       "10004            W. 173rd \n",
       "100044           E 38th St\n",
       "100048          West 63rd \n",
       "10005       East 56th St..\n",
       "100051          East 34th \n",
       "100052            1st Ave.\n",
       "100053           Thayer St\n",
       "100055         West 106th \n",
       "100058                1st \n",
       "Name: new, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new'] = df['display_address'].apply(lambda x: x.replace('Street', '').replace('Avenue', ''))\n",
    "df['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dual': True, 'loss': 'hinge'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(loss = 'hinge', dual = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting code\n",
      "finished training\n",
      "Log loss from sklearn 0.994068892176\n",
      "starting code\n",
      "finished training\n",
      "Log loss from sklearn 0.92945600308\n",
      "starting code\n",
      "finished training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [u'low' u'medium']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d57c54da96bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#loss = logloss(pred, test_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msklearn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Log loss from sklearn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#scores = cross_val_score(pipeline, df, df['interest_level'], cv=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1652\u001b[0m                              \"y_true: {2}\".format(transformed_labels.shape[1],\n\u001b[1;32m   1653\u001b[0m                                                   \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m                                                   lb.classes_))\n\u001b[0m\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             raise ValueError('The number of classes in labels is different '\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [u'low' u'medium']"
     ]
    }
   ],
   "source": [
    "#pg = dict(clf__verbose = ['True'])\n",
    "pg = {'clf__learning_rate' : learning, 'clf__n_estimators' : estimators}\n",
    "\n",
    "#import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from Transformers import TextTransformer, ColumnExtractor\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "a = TextTransformer('description', max_features=3000)\n",
    "b = TextTransformer('features', max_features=3000)\n",
    "c = TextTransformer('street_address', max_features = 3000)\n",
    "d = TextTransformer('display_address', max_features = 3000)\n",
    "pipeline = Pipeline([\n",
    "        ('test', FeatureUnion\n",
    "         ([\n",
    "            #('description', a ), # can pass in either a pipeline\n",
    "            #('features', b),\n",
    "            #('street', c),\n",
    "            #('display', d),\n",
    "            #('lat_long', ColumnExtractor(['latitude', 'longitude'])),\n",
    "            ('year', DateExtractor('created', year = True)),\n",
    "            ('month', DateExtractor('created', month = True)),\n",
    "            ('day', DateExtractor('created', day = True))\n",
    "                    \n",
    "        ])),\n",
    "    #('clf',xgboost.XGBClassifier(silent = False))\n",
    "    ('clf',SVC(probability = True))\n",
    "    ])\n",
    "for train_index, test_index in skf.split(df, df['interest_level']):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df.iloc[train_index], df['interest_level'].iloc[train_index]\n",
    "    test, test_labels = df.iloc[test_index], df['interest_level'].iloc[test_index]\n",
    "    pred = pipeline.fit(train,train_labels)\n",
    "    print 'finished training'\n",
    "    pred = (pred.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "#scores = cross_val_score(pipeline, df, df['interest_level'], cv=2)\n",
    "#regr = \n",
    "#search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "#pg = {'clf__C': [0.1,0.2]}\n",
    "\n",
    "#print pipeline.get_params().keys()\n",
    "#grid = GridSearchCV(pipeline, param_grid = pg, cv = 2, verbose = 10)\n",
    "#grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_transform(content_lst):\n",
    "    return ' '.join(content_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "param_grid = dict(learning_rate = learning, n_estimators = estimators)\n",
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(silent = False)\n",
    "    search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "    res = search.fit(train, train_labels)\n",
    "    results.append(res)\n",
    "\n",
    "    #regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (res.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(n_estimators  = 300, silent = False)\n",
    "    regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (regr.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json('data/test.json')\n",
    "listing_ids = test_df['listing_id']\n",
    "test_df['description'] = test_df['description'].apply(clean_html_tags)\n",
    "test_df = test_df[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'description', 'features']]\n",
    "test_df['description'] = test_df['description'].astype(unicode)\n",
    "test_df['features'] = test_df['features'].astype(unicode)\n",
    "test_df_bow = tf_transformer.transform(test_df['description'])\n",
    "test_df_bow = pd.DataFrame(test_df_bow.todense())\n",
    "test_df = test_df.drop(['description'], axis = 1)\n",
    "test_df_bow_features = tf_transformer_features.transform(test_df['features'])\n",
    "test_df_bow_features = pd.DataFrame(test_df_bow_features.todense())\n",
    "test_df_bow_features.columns = names\n",
    "test_df = test_df.drop(['features'], axis = 1)\n",
    "test_df = test_df.join(test_df_bow)\n",
    "test_df = test_df.join(test_df_bow_features)\n",
    "    \n",
    "pred = regr.predict_proba(test_df)\n",
    "\n",
    "pred = pd.DataFrame(pred, columns = ['high', 'medium', 'low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred['listing_id'] = listing_ids.values\n",
    "pred = pred[['listing_id', 'high', 'medium', 'low']]\n",
    "pred.to_csv('test_raw_xgboost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = regr.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epochs')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================================\n",
    "Concatenating multiple feature extraction methods\n",
    "=================================================\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a\n",
    "dataset. Often it is beneficial to combine several methods to obtain good\n",
    "performance. This example shows how to use ``FeatureUnion`` to combine\n",
    "features obtained by PCA and univariate selection.\n",
    "\n",
    "Combining features using this transformer has the benefit that it allows\n",
    "cross validation and grid searches over the whole process.\n",
    "\n",
    "The combination used in this example is not particularly helpful on this\n",
    "dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print iris.data\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "#param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  #features__univ_select__k=[1, 2],\n",
    "                  #svm__C=[0.1, 1, 10])\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,col, max_features = 10):\n",
    "        self.tfidfVectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._vectorizer = None\n",
    "        self._column = 'description'\n",
    "\n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        self._vectorizer = self.tfidfVectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return self._vectorizer.transform(df[self._column]).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "len(df)\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "a = TextTransformer('testing')\n",
    "b = TextTransformer('features', max_features=10)\n",
    "a = FeatureUnion([(\"pca\", a), ('test', b)])\n",
    "pipe = Pipeline([('description', a)])\n",
    "X = df\n",
    "y = df['interest_level'].values\n",
    "pipe.fit(X,y)\n",
    "# pg = {'clf__C': [0.1,1]}\n",
    "# grid = GridSearchCV(pipeline, param_grid= pg ,cv = 2)\n",
    "# grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df[['latitude', 'longitude', 'interest_level']]\n",
    "a['interest_level'] = a['interest_level'].apply(repl)\n",
    "#a = a.pivot_table('latitude', 'longitude', 'interest_level', aggfunc='sum')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_lat = np.max(df['latitude'])\n",
    "min_lat = np.min(df['latitude'])\n",
    "mean = np.mean(df['latitude'])\n",
    "std =  np.std(df['latitude'])\n",
    "width = 3 * std\n",
    "(n, bin, patch) = plt.hist(df['latitude'], bins = [min_lat, mean - width, mean , mean + width, max_lat])\n",
    "print n \n",
    "print bin\n",
    "print patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df[['latitude', 'longitude', 'interest_level']]\n",
    "df1.head\n",
    "df2 = df1[df1['latitude'] >41]\n",
    "sns.boxplot(y=df2['latitude'], data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('latitude', 'longitude', data = df1, hue = 'interest_level', fit_reg = False, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1[ (df1['longitude'] > -74.1) & (df1['longitude'] <-73.8)]\n",
    "df1 = df1[(df1['latitude'] > 40.5) & (df1['latitude'] <40.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repl(label):\n",
    "    if label == 'low':\n",
    "        return 1\n",
    "    elif label == 'medium':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['latitude'] = pd.cut(df['latitude'], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['longitude'] = pd.cut(df['longitude'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
