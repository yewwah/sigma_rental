{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sb \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [300]\n",
    "learning = [0.3,0.4,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    print type(x)\n",
    "    if type(x) == list:\n",
    "        return ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "df = df[:20]\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column, max_features):\n",
    "        self.tfidfVectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._vectorizer = None\n",
    "        self._column = column\n",
    "        \n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        if self._column == 'features':\n",
    "            df[self._column] = df[self._column].apply(check)\n",
    "        self._vectorizer = self.tfidfVectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        if self._column == 'features':\n",
    "            df[self._column] = df[self._column].apply(check)\n",
    "#        return self._vectorizer.transform(df[self._column].apply(self._clean_html_tags))\n",
    "        return self._vectorizer.transform(df[self._column])\n",
    "\n",
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        print 'col'\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        return df[self.cols].values\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class DateExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, day = False, month= False, year = False):\n",
    "        self.cols = cols\n",
    "        self.day = day\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "        self.index = None\n",
    "        \n",
    "    def transform(self, df, y = None):\n",
    "        frame = self._fit_date(df[self.cols])\n",
    "        cols_names = frame.columns.values\n",
    "        \n",
    "        #get the columns that interesect to account for unseen labels\n",
    "        intersect = set.intersection(set(cols_names), set(self.index))\n",
    "        \n",
    "        #get the differences to account for unseen labels\n",
    "        diff = set.difference(set(self.index), set(cols_names))\n",
    "        frame = frame[list(intersect)]\n",
    "        frame = pd.concat((frame, pd.DataFrame(columns = list(diff))))\n",
    "        frame.fillna(0, inplace=True)\n",
    "        return frame\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = self._fit_date(X[self.cols])\n",
    "        self.index = df.columns.values\n",
    "        return self\n",
    "    \n",
    "    def _fit_date(self, X):\n",
    "        if self.day:\n",
    "            prefix = 'day'\n",
    "            when = pd.DatetimeIndex(X).day\n",
    "        elif self.month:\n",
    "            prefix = 'month'\n",
    "            when = pd.DatetimeIndex(X).month\n",
    "        else:\n",
    "            prefix = 'year'\n",
    "            when = pd.DatetimeIndex(X).year\n",
    "        \n",
    "        frame = pd.get_dummies(when, prefix = prefix)\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "init\n",
      "init\n",
      "init\n",
      "starting code\n",
      "finished training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342']\nexpected f350, f343, f345, f344, f347, f346, f349, f348 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-3bea37b48675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'finished training'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m#loss = logloss(pred, test_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msklearn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[1;32m    475\u001b[0m         class_probs = self.booster().predict(test_dmatrix,\n\u001b[1;32m    476\u001b[0m                                              \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                                              ntree_limit=ntree_limit)\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0moption_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;36m0x02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yewwah-ubuntu/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1179\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342']\nexpected f350, f343, f345, f344, f347, f346, f349, f348 in input data"
     ]
    }
   ],
   "source": [
    "#pg = dict(clf__verbose = ['True'])\n",
    "pg = {'clf__learning_rate' : learning, 'clf__n_estimators' : estimators}\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "#from Transformers import TextTransformer, ColumnExtractor\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in skf.split(df, df['interest_level']):\n",
    "    a = TextTransformer('description', max_features=3000)\n",
    "    b = TextTransformer('features', max_features=3000)\n",
    "    c = TextTransformer('street_address', max_features = 3000)\n",
    "    d = TextTransformer('display_address', max_features = 3000)\n",
    "    pipeline = Pipeline([\n",
    "        ('test', FeatureUnion\n",
    "         ([\n",
    "            ('description', a ), # can pass in either a pipeline\n",
    "            #('features', b),\n",
    "            #('street', c),\n",
    "            #('display', d),\n",
    "            #('lat_long', ColumnExtractor(['latitude', 'longitude'])),\n",
    "            #('year', DateExtractor('created', year = True)),\n",
    "            #('month', DateExtractor('created', month = True)),\n",
    "            #('day', DateExtractor('created', day = True))\n",
    "                    \n",
    "        ])),\n",
    "    ('clf',xgboost.XGBClassifier(silent = False, n_estimators = 300, learning_rate = 0.2, max_depth = 5))\n",
    "    #('clf',SVC(probability = True))\n",
    "    ])\n",
    "    print 'starting code'\n",
    "    train, train_labels = df.iloc[train_index], df['interest_level'].iloc[train_index]\n",
    "    test, test_labels = df.iloc[test_index], df['interest_level'].iloc[test_index]\n",
    "    pred = pipeline.fit(train,train_labels)\n",
    "    print 'finished training'\n",
    "    pred = (pred.predict_proba(test))\n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    break\n",
    "#scores = cross_val_score(pipeline, df, df['interest_level'], cv=2)\n",
    "#regr = \n",
    "#search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "#pg = {'clf__C': [0.1,0.2]}\n",
    "\n",
    "#print pipeline.get_params().keys()\n",
    "#grid = GridSearchCV(pipeline, param_grid = pg, cv = 2, verbose = 10)\n",
    "#grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_transform(content_lst):\n",
    "    return ' '.join(content_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "param_grid = dict(learning_rate = learning, n_estimators = estimators)\n",
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(silent = False)\n",
    "    search = GridSearchCV(regr, param_grid, scoring = 'neg_log_loss', n_jobs = -1)\n",
    "    res = search.fit(train, train_labels)\n",
    "    results.append(res)\n",
    "\n",
    "    #regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (res.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(df_cleaned, df_target):\n",
    "    print 'starting code'\n",
    "    train, train_labels = df_cleaned.iloc[train_index], df_target.iloc[train_index]\n",
    "    tf_transformer = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 5000)\n",
    "    tf_transformer_features = TfidfVectorizer(use_idf=False,  stop_words = 'english', \n",
    "                                     tokenizer = custom_tokenizer, analyzer = 'word', max_features = 3000)\n",
    "    \n",
    "    train_bow = tf_transformer.fit_transform(train['description'])\n",
    "    train = train.drop(['description'], axis = 1)\n",
    "    train_bow_features = tf_transformer_features.fit_transform(train['features'])\n",
    "    train = train.drop(['features'], axis = 1)\n",
    "    train_bow = pd.DataFrame(train_bow.todense())\n",
    "    names = [str(x) for x in range(5000,5000 + train_bow_features.shape[1])]\n",
    "    train_bow_features = pd.DataFrame(train_bow_features.todense())\n",
    "    train_bow_features.columns = names\n",
    "    \n",
    "    train = train.join(train_bow)\n",
    "    train = train.join(train_bow_features)\n",
    "    train.fillna(0, inplace = True)\n",
    "    print train.shape\n",
    "    print 'Building the model'\n",
    "     \n",
    "    test, test_labels = df_cleaned.iloc[test_index], df_target.iloc[test_index]\n",
    "    test_bow = tf_transformer.transform(test['description'])\n",
    "    test_bow = pd.DataFrame(test_bow.todense())\n",
    "    test_bow_features = tf_transformer_features.transform(test['features'])\n",
    "    test_bow_features = pd.DataFrame(test_bow_features.todense())\n",
    "    test_bow_features.columns = names\n",
    "    test = test.drop(['description'], axis = 1)\n",
    "    test = test.drop(['features'], axis = 1)\n",
    "     \n",
    "    #train.fillna('0', inplace = True)\n",
    "    test = test.join(test_bow)\n",
    "    test = test.join(test_bow_features)\n",
    "    eval_set = [(train, train_labels), (test, test_labels)]\n",
    "    regr = xgboost.XGBClassifier(n_estimators  = 300, silent = False)\n",
    "    regr.fit(train, train_labels, eval_metric = 'mlogloss', eval_set = eval_set, verbose = True)\n",
    "    \n",
    "    \n",
    "    #test.fillna('0', inplace = True)\n",
    "    #regr = linear_model.LogisticRegression(class_weight = 'balanced', probability = True)\n",
    "\n",
    "    print 'finished training'\n",
    "    pred = (regr.predict_proba(test))\n",
    "    \n",
    "    #loss = logloss(pred, test_labels)\n",
    "    sklearn_loss = log_loss(test_labels, pred)\n",
    "    print 'Log loss from sklearn', sklearn_loss\n",
    "    \n",
    "    #print confusion_matrix(pred, test_labels)\n",
    "    #print accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json('data/test.json')\n",
    "listing_ids = test_df['listing_id']\n",
    "test_df['description'] = test_df['description'].apply(clean_html_tags)\n",
    "test_df = test_df[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'description', 'features']]\n",
    "test_df['description'] = test_df['description'].astype(unicode)\n",
    "test_df['features'] = test_df['features'].astype(unicode)\n",
    "test_df_bow = tf_transformer.transform(test_df['description'])\n",
    "test_df_bow = pd.DataFrame(test_df_bow.todense())\n",
    "test_df = test_df.drop(['description'], axis = 1)\n",
    "test_df_bow_features = tf_transformer_features.transform(test_df['features'])\n",
    "test_df_bow_features = pd.DataFrame(test_df_bow_features.todense())\n",
    "test_df_bow_features.columns = names\n",
    "test_df = test_df.drop(['features'], axis = 1)\n",
    "test_df = test_df.join(test_df_bow)\n",
    "test_df = test_df.join(test_df_bow_features)\n",
    "    \n",
    "pred = regr.predict_proba(test_df)\n",
    "\n",
    "pred = pd.DataFrame(pred, columns = ['high', 'medium', 'low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred['listing_id'] = listing_ids.values\n",
    "pred = pred[['listing_id', 'high', 'medium', 'low']]\n",
    "pred.to_csv('test_raw_xgboost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10           Metropolitan \n",
       "10000            Columbus \n",
       "100004               W 13 \n",
       "100007          East 49th \n",
       "100013         West 143rd \n",
       "100014          West 18th \n",
       "100016         West 107th \n",
       "100020          West 21st \n",
       "100026    Hamilton Terrace\n",
       "100027          522 E 11th\n",
       "100030               York \n",
       "10004            W. 173rd \n",
       "100044           E 38th St\n",
       "100048          West 63rd \n",
       "10005       East 56th St..\n",
       "100051          East 34th \n",
       "100052            1st Ave.\n",
       "100053           Thayer St\n",
       "100055         West 106th \n",
       "100058                1st \n",
       "Name: new, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new'] = df['display_address'].apply(lambda x: x.replace('Street', '').replace('Avenue', ''))\n",
    "df['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = regr.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "\n",
    "pyplot.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('epochs')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================================\n",
    "Concatenating multiple feature extraction methods\n",
    "=================================================\n",
    "\n",
    "In many real-world examples, there are many ways to extract features from a\n",
    "dataset. Often it is beneficial to combine several methods to obtain good\n",
    "performance. This example shows how to use ``FeatureUnion`` to combine\n",
    "features obtained by PCA and univariate selection.\n",
    "\n",
    "Combining features using this transformer has the benefit that it allows\n",
    "cross validation and grid searches over the whole process.\n",
    "\n",
    "The combination used in this example is not particularly helpful on this\n",
    "dataset and is only used to illustrate the usage of FeatureUnion.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print iris.data\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "#param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  #features__univ_select__k=[1, 2],\n",
    "                  #svm__C=[0.1, 1, 10])\n",
    "pipeline.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,col, max_features = 10):\n",
    "        self.tfidfVectorizer = TfidfVectorizer(use_idf=False, stop_words='english',\n",
    "                                               tokenizer=self._custom_tokenizer, analyzer='word',\n",
    "                                               max_features=max_features)\n",
    "        self._vectorizer = None\n",
    "        self._column = 'description'\n",
    "\n",
    "    def _custom_tokenizer(self, string):\n",
    "        # string = re.sub('^[\\w]', '', string)\n",
    "        tokens = nltk.word_tokenize(string)\n",
    "        cleaned = [x if not x.isdigit() else '_NUM_' for x in tokens]\n",
    "        return [str(x.encode('utf-8')) for x in cleaned if (x.isalpha() or x == '_NUM_')]\n",
    "\n",
    "    def _clean_html_tags(self, content):\n",
    "        return BeautifulSoup(content, 'lxml').text\n",
    "\n",
    "    def fit(self, df, y = None):\n",
    "        self._vectorizer = self.tfidfVectorizer.fit(df[self._column].apply(self._clean_html_tags))\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return self._vectorizer.transform(df[self._column]).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.json', encoding = 'utf-8', dtype = {'description': str})\n",
    "len(df)\n",
    "#df = df[['description', 'interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "a = TextTransformer('testing')\n",
    "b = TextTransformer('features', max_features=10)\n",
    "a = FeatureUnion([(\"pca\", a), ('test', b)])\n",
    "pipe = Pipeline([('description', a)])\n",
    "X = df\n",
    "y = df['interest_level'].values\n",
    "pipe.fit(X,y)\n",
    "# pg = {'clf__C': [0.1,1]}\n",
    "# grid = GridSearchCV(pipeline, param_grid= pg ,cv = 2)\n",
    "# grid.fit(df, df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df[['latitude', 'longitude', 'interest_level']]\n",
    "a['interest_level'] = a['interest_level'].apply(repl)\n",
    "#a = a.pivot_table('latitude', 'longitude', 'interest_level', aggfunc='sum')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_lat = np.max(df['latitude'])\n",
    "min_lat = np.min(df['latitude'])\n",
    "mean = np.mean(df['latitude'])\n",
    "std =  np.std(df['latitude'])\n",
    "width = 3 * std\n",
    "(n, bin, patch) = plt.hist(df['latitude'], bins = [min_lat, mean - width, mean , mean + width, max_lat])\n",
    "print n \n",
    "print bin\n",
    "print patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df[['latitude', 'longitude', 'interest_level']]\n",
    "df1.head\n",
    "df2 = df1[df1['latitude'] >41]\n",
    "sns.boxplot(y=df2['latitude'], data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('latitude', 'longitude', data = df1, hue = 'interest_level', fit_reg = False, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1[ (df1['longitude'] > -74.1) & (df1['longitude'] <-73.8)]\n",
    "df1 = df1[(df1['latitude'] > 40.5) & (df1['latitude'] <40.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repl(label):\n",
    "    if label == 'low':\n",
    "        return 1\n",
    "    elif label == 'medium':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['latitude'] = pd.cut(df['latitude'], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['longitude'] = pd.cut(df['longitude'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
